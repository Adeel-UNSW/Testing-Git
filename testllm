def createVectorIndex(path):
    max_input = 4096
    tokens = 256
    chunk_size = 600
    max_chunk_overlap = 20

    prompthelper = PromptHelper(max_input,tokens,max_chunk_overlap,chunk_size_limit=chunk_size)

    #define LLM
    llmPredictor = LLMPredictor(llm=OpenAI(temperature=0,model_name="text-ada-001",max_tokens=tokens))

    #load data
    docs = SimpleDirectoryReader(path).load_data()

    #create vector index
    service_context = ServiceContext.from_defaults(llm_predictor=llmPredictor,prompt_helper= prompthelper)
    vectorIndex = GPTSimpleVectorIndex.from_documents(documents=docs,service_context=service_context)
    vectorIndex.save_to_disk('vectorIndex1.json')
    return vectorIndex
    
    
    
    vectorIndex = createVectorIndex('know')
    
    
    def answerMe(vectorIndex):
    vIndex = GPTSimpleVectorIndex.load_from_disk(vectorIndex)
    while True:
        prompt = input('Please ask: ')
        response = vIndex.query(prompt,response_mode="compact")
        print(f"Response: {response} \n")
